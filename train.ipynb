{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is from https://fehiepsi.github.io/blog/grapheme-to-phoneme/\n",
    "# Based on https://github.com/SeanNaren/deepspeech.pytorch/blob/master/decoder.py.\n",
    "import Levenshtein  # https://github.com/ztane/python-Levenshtein/\n",
    "\n",
    "\n",
    "def phoneme_error_rate(p_seq1, p_seq2):\n",
    "    # To calculate the PER\n",
    "    p_vocab = set(p_seq1 + p_seq2)\n",
    "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
    "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
    "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
    "    return Levenshtein.distance(''.join(c_seq1),\n",
    "                                ''.join(c_seq2)) / len(c_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are so that we can map the phonemes to integers and then back to phonemes\n",
    "phones = [\n",
    "    \"aa\",\n",
    "    \"ae\",\n",
    "    \"ah\",\n",
    "    \"ay\",\n",
    "    \"aw\",\n",
    "    \"b\",\n",
    "    \"ch\",\n",
    "    \"d\",\n",
    "    \"dh\",\n",
    "    \"dx\",\n",
    "    \"eh\",\n",
    "    \"er\",\n",
    "    \"ey\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"hh\",\n",
    "    \"ih\",\n",
    "    \"k\",\n",
    "    \"iy\",\n",
    "    \"jh\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"ng\",\n",
    "    \"oy\",\n",
    "    \"ow\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"sh\",\n",
    "    \"t\",\n",
    "    \"th\",\n",
    "    \"uw\",\n",
    "    \"uh\",\n",
    "    \"p\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"sil\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"MFCCS2\")\n",
    "\n",
    "trainX = np.load(path / \"X_train.npy\")\n",
    "trainY = np.load(path / \"y_train.npy\")\n",
    "\n",
    "testX = np.load(path / \"X_test.npy\")\n",
    "testY = np.load(path / \"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539007, 40, 45)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# From https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "char_to_int = dict((c, i) for i, c in enumerate(phones))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(phones))\n",
    "trainY = [char_to_int[phone] for phone in trainY]\n",
    "testY = [char_to_int[phone] for phone in testY]\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "trainY = to_categorical(trainY)\n",
    "testY= to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape((-1, trainX.shape[1], trainX.shape[2], 1))\n",
    "testX = testX.reshape((-1, testX.shape[1], testX.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(\n",
    "        trainX, trainY, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lars/Projects/tensorflowgputest/venv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=39)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 36, 41, 150)       3900      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 7, 150)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6300)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              6301000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 39)                39039     \n",
      "=================================================================\n",
      "Total params: 7,344,939\n",
      "Trainable params: 7,344,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 388084 samples, validate on 43121 samples\n",
      "Epoch 1/11\n",
      "388084/388084 [==============================] - 41s 104us/step - loss: 2.5531 - accuracy: 0.4104 - val_loss: 1.8058 - val_accuracy: 0.4797\n",
      "Epoch 2/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.7376 - accuracy: 0.4950 - val_loss: 1.6920 - val_accuracy: 0.5066\n",
      "Epoch 3/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.6645 - accuracy: 0.5095 - val_loss: 1.6663 - val_accuracy: 0.5095\n",
      "Epoch 4/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.6200 - accuracy: 0.5185 - val_loss: 1.6607 - val_accuracy: 0.5121\n",
      "Epoch 5/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.5852 - accuracy: 0.5264 - val_loss: 1.5897 - val_accuracy: 0.5247\n",
      "Epoch 6/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.5642 - accuracy: 0.5307 - val_loss: 1.5593 - val_accuracy: 0.5338\n",
      "Epoch 7/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.5422 - accuracy: 0.5353 - val_loss: 1.5458 - val_accuracy: 0.5357\n",
      "Epoch 8/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.5238 - accuracy: 0.5395 - val_loss: 1.5422 - val_accuracy: 0.5349\n",
      "Epoch 9/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.5083 - accuracy: 0.5422 - val_loss: 1.5327 - val_accuracy: 0.5382\n",
      "Epoch 10/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.4896 - accuracy: 0.5477 - val_loss: 1.5320 - val_accuracy: 0.5381\n",
      "Epoch 11/11\n",
      "388084/388084 [==============================] - 40s 103us/step - loss: 1.4784 - accuracy: 0.5497 - val_loss: 1.5401 - val_accuracy: 0.5388\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "output_dim = len(trainY[0])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(150,\n",
    "                        (5, 5), \n",
    "                        input_shape=(trainX.shape[1], trainX.shape[2], 1),\n",
    "                        activation = 'relu'))\n",
    "model.add(MaxPooling2D((6,6),\n",
    "                      padding=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "    \n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(output_dim=output_dim, activation = 'softmax'))\n",
    "model.summary() \n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "      metrics=[\"accuracy\"],\n",
    "      optimizer=Adam(lr=0.001)\n",
    ")\n",
    "\n",
    "stats = model.fit(trainX, trainY,\n",
    "        shuffle=True,\n",
    "        batch_size=1024,\n",
    "        epochs=11,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107802/107802 [==============================] - 6s 53us/step\n",
      "107802/107802 [==============================] - 6s 56us/step\n",
      "PER is : 0.461234485445539 Accuracy is : [1.544046977864942, 0.5382367968559265]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(x=testX, batch_size=512, verbose=1)\n",
    "print(f\"PER is : {phoneme_error_rate(predictions.argmax(axis=1), testY.argmax(axis=1))} Accuracy is : {model.evaluate(x=testX, y = testY, batch_size=512, verbose=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dim = len(trainY[0])\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(256, activation = 'relu'))\n",
    "model2.add(Dense(256, activation = 'relu'))\n",
    "model2.add(Dense(output_dim=output_dim, activation = 'relu'))\n",
    "model2.add(Dense(output_dim=output_dim))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(), \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "stats = model2.fit(trainX, trainY,\n",
    "    shuffle=True,\n",
    "    batch_size=2048,\n",
    "    nb_epoch=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=testX, batch_size=512, verbose=1)\n",
    "print(f\"PER is : {phoneme_error_rate(predictions.argmax(axis=1), testY.argmax(axis=1))} Accuracy is : {model.evaluate(x=testX, y = testY, batch_size=512, verbose=1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mainly used to get the confusion matrix\n",
    "# This code is copied from somewhere, but I can sadly not figure out where that was from.\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(testX[0].shape)\n",
    "print(trainX[0].shape)\n",
    "predictions = model.predict(x=testX, batch_size=512, verbose=1)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1)))\n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, stats.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, stats.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (Simple NN)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model2.predict(x=testX, batch_size=512, verbose=1)\n",
    "print(f\"PER is : {phoneme_error_rate(predictions.argmax(axis=1), testY.argmax(axis=1))} Accuracy is : {model2.evaluate(x=testX, y = testY, batch_size=512, verbose=1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PER of tests\n",
    "#### CNN Test Result 8.24\n",
    "0.6514674425125122\n",
    "\n",
    "0.6436238884925842\n",
    "\n",
    "0.6488529443740845\n",
    "\n",
    "#### CNN Horizontal Test Result (40x1) 2.3M\n",
    "0.6527880430221558\n",
    "\n",
    "0.6515655517578125\n",
    "\n",
    "0.6558755040168762\n",
    "\n",
    "#### Small CNN Horizontal Test Result (40x1) 181k  pars\n",
    "0.641758918762207\n",
    "\n",
    "0.6443377733230591\n",
    "\n",
    "0.6448642611503601\n",
    "\n",
    "#### Small CNN 181k params (40x8)\n",
    "0.6403490900993347\n",
    "\n",
    "0.6396352052688599\n",
    "\n",
    "0.6392336487770081\n",
    "\n",
    "#### DNN Test Result 6,8M\n",
    "0.6274282336235046\n",
    "\n",
    "0.6198791861534119\n",
    "\n",
    "0.6232432126998901\n",
    "\n",
    "#### DNN 256 * 2 567k\n",
    "0.5806347727775574\n",
    "\n",
    "0.5770119428634644\n",
    "\n",
    "0.575503945350647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn = [0.6514674425125122,0.6436238884925842,0.6488529443740845]\n",
    "\n",
    "cnn2 = [0.6527880430221558, 0.6515655517578125, 0.6558755040168762]\n",
    "\n",
    "cnn3 = [0.6403490900993347, 0.6396352052688599, 0.6392336487770081]\n",
    "\n",
    "cnn4 = [0.641758918762207, 0.6443377733230591, 0.6448642611503601]\n",
    "\n",
    "dnn = [0.6274282336235046, 0.6198791861534119, 0.6232432126998901]\n",
    "\n",
    "dnn2 = [0.5806347727775574, 0.5770119428634644, 0.575503945350647]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
